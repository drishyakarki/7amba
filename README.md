# 7amba
Welcome to the official repository for 7amba, a project developed as part of my final year college research. This project initially focused on exploring the innovative Mamba Architectureâ€”an emerging model architecture that shows competitive performance compared to the widely-adopted transformer model.

## Project Overview
Our research began with an investigation into the capabilities of the original Mamba Architecture. We aimed to push its boundaries by integrating attention layers after every seven consecutive Mamba layers, hoping to enhance the model's performance further.

However, during our research, the original authors of Mamba released the Mamba-2 paper, which incorporated similar concepts to those we were exploring. Given this development, we decided to pivot our research focus. Instead of continuing on the same path, we shifted our efforts towards fine-tuning pretrained Mamba models on downstream chat tasks, an area that still offers plenty of opportunities for innovation and exploration.

## Current Focus: Fine-Tuning Mamba Models
This repository now houses the code for fine-tuning Mamba models on chat datasets. The codebase is designed to be flexible and can be easily adapted for other tasks as well. By making this code public, we hope to assist others who are working on fine-tuning pretrained Large Language Models (LLMs), particularly those based on the Mamba architecture.

## Future Work: HAMBA
In parallel with this project, we are also working on a paper titled HAMBA, which further investigates the capabilities of the Mamba architecture. Once completed, we will release the associated code in the [repo](https://github.com/AssistantsLab/HAMBA).

## Getting Started
Simply clone the repo and get started with your finetuning. You have to adjust your dataset path.

## Sample outputs

Here are some of the outputs generated from the model.

![First image](images/img.png)

This is the first image showing the output of the model.

![Second image](images/img2.png)

This is the second image showing the output of the model.